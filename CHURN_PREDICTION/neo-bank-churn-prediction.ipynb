{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T11:05:38.001454Z","iopub.execute_input":"2025-06-17T11:05:38.001722Z","iopub.status.idle":"2025-06-17T11:05:38.384658Z","shell.execute_reply.started":"2025-06-17T11:05:38.001695Z","shell.execute_reply":"2025-06-17T11:05:38.383706Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# **IMPORTS**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport ast\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T11:16:56.262806Z","iopub.execute_input":"2025-06-17T11:16:56.263556Z","iopub.status.idle":"2025-06-17T11:16:57.807249Z","shell.execute_reply.started":"2025-06-17T11:16:56.263524Z","shell.execute_reply":"2025-06-17T11:16:57.806345Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **ALL MODULAR FUNCTIONS**","metadata":{}},{"cell_type":"code","source":"1. # loading and preprocessing the data\ndef load_and_preprocess_data(file_paths):\n    dfs = []\n    for file_path in file_paths:\n        df = pd.read_parquet(file_path)\n        dfs.append(df)\n    df = pd.concat(dfs, ignore_index=True)\n    \n    #Drop address\n    df = df.drop(columns=['address'])\n    \n    #Handle churn due to fraud as a boolean\n    df['churn_due_to_fraud'] = df['churn_due_to_fraud'].astype(int)\n    \n    #Transform date columns into datetime objects.\n    df['date'] = pd.to_datetime(df['date'])\n    df['date_of_birth'] = pd.to_datetime(df['date_of_birth'])\n    \n    return df\n\n# who are the churners?\n\ndef create_target_variable(df):\n    df['churn'] = 0\n    # churn = 1 for customer last interaction 2023-01-01\n    filtered_df = df[df['date'] < '2023-01-01']\n    idx = filtered_df.groupby('customer_id')['date'].idxmax()\n    df.loc[idx, 'churn'] = 1\n    return df\n\n\n# some features\ndef feature_engineering(df):\n    # Time-based features\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['month'] = df['date'].dt.month\n    df['quarter'] = df['date'].dt.quarter\n    df['year'] = df['date'].dt.year\n    df['days_since_start'] = (df['date'] - df.groupby('customer_id')['date'].transform('min')).dt.days\n    df['days_since_last_activity'] = (df.groupby('customer_id')['date'].transform('max') - df['date']).dt.days\n    df['age'] = ((df['date'] - df['date_of_birth']).dt.days / 365.25).astype(int)\n    \n    # Interaction Features\n    df['total_transfer_volume'] = df['bank_transfer_in_volume'] + df['bank_transfer_out_volume']\n    df['crypto_transfer_volume'] = df['crypto_in_volume'] + df['crypto_out_volume']\n    df['volume_ratio'] = (df['crypto_transfer_volume'] + 1) / (df['total_transfer_volume'] + 1)\n\n  # Touchpoint features\n    # Convert touchpoints string to list\n    df['touchpoints'] = df['touchpoints'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    # parse the list and get all type of touchpoints\n    unique_touchpoints = set()\n    for touchpoints_list in df['touchpoints']:\n        if isinstance(touchpoints_list, list):\n            unique_touchpoints.update(touchpoints_list) \n            \n    # touchpoint_email = 2, touchpoint_sms = 1 etc per row. \n    for touchpoint in unique_touchpoints:\n        df[f'touchpoint_{touchpoint}'] = df['touchpoints'].apply(lambda x: x.count(touchpoint) if isinstance(x,list) else 0)\n\n    #csat features\n    def extract_csat_scores(csat_dict, channel):\n      if isinstance(csat_dict, str):\n          try:\n              csat_dict = ast.literal_eval(csat_dict)\n          except (ValueError, SyntaxError):\n              return np.nan\n      if isinstance(csat_dict,dict) and channel in csat_dict:\n         return csat_dict.get(channel)\n      return np.nan\n\n    unique_channels = set()\n    for csat_dict in df['csat_scores']:\n      if isinstance(csat_dict, str):\n         try:\n            csat_dict = ast.literal_eval(csat_dict)\n         except (ValueError, SyntaxError):\n            continue\n      if isinstance(csat_dict,dict):\n         unique_channels.update(csat_dict.keys())\n\n    # csat score for diff channels.\n    for channel in unique_channels:\n        df[f'csat_{channel}'] = df['csat_scores'].apply(lambda x : extract_csat_scores(x, channel))\n    \n    #Drop nested columns\n    df=df.drop(columns=['touchpoints','csat_scores'])\n\n    return df\n\n\n# preprocessing pipeline\ndef get_preprocessing_pipeline(numerical_features, categorical_features):\n    # Numerical features pipeline\n    numerical_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ])\n    #Categorical Features pipeline\n    categorical_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('numerical', numerical_pipeline, numerical_features),\n            ('categorical', categorical_pipeline, categorical_features)\n        ],\n        remainder = 'passthrough'\n    )\n    \n    return preprocessor\n\n\n# hyperparams tuning\ndef get_model(params):\n    return xgb.XGBClassifier(**params, objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n\n\n\ndef get_model(params):\n    return xgb.XGBClassifier(**params, objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n\n\n\n\n#2. Model Training and Evaluation\ndef train_models(df, preprocessor, numerical_features, categorical_features):\n  \n  df_train = df[df['date'].dt.year < 2023]\n  df_train = df_train.sort_values(by='date')\n  X_train_all = df_train.drop(columns=['churn','customer_id', 'name', 'date', 'date_of_birth', 'Id'])\n  y_train_all = df_train['churn']\n  \n  # Preprocess data\n  X_train_all = preprocessor.fit_transform(X_train_all)\n\n  # Split training and validation data\n  X_train_all, X_val_all, y_train_all, y_val_all = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42, shuffle = False)\n\n  xgb_params = {\n      'n_estimators': 500,\n      'learning_rate': 0.05,\n      'max_depth': 5,\n      'device': 'cuda',\n      'tree_method': 'hist',\n      'subsample': 0.8,\n      'colsample_bytree': 0.8,\n      'random_state': 42,\n      'early_stopping_rounds': 10\n  }\n  \n  model_all = get_model(xgb_params)\n\n  model_all.fit(X_train_all, y_train_all, eval_set=[(X_val_all, y_val_all)], verbose=50)\n\n # return model_all\n  return model_all, _, _\n\ndef predict_with_orchestration(df, preprocessor, model_all):\n  X_test = df.drop(columns=['customer_id', 'name', 'date', 'date_of_birth', 'Id'])\n  X_test = preprocessor.transform(X_test)\n  \n  preds = np.zeros(X_test.shape[0])\n\n  for i, row in df.iterrows():\n      # predict proba expects (n_sample, n_feat) and returns (prob(0), prob(1))\n      preds[i]=model_all.predict_proba(X_test[i].reshape(1,-1))[:,1]\n\n  return pred\n\n\ndef evaluate_model(y_true, y_pred):\n    logloss = log_loss(y_true, y_pred)\n    return logloss\n\n\n# 3. Prediction and Submission\ndef create_submission_file(test_df, predictions, file_path):\n    submission_df = pd.DataFrame({'Id': test_df['Id'], 'churn': predictions})\n    submission_df.to_csv(file_path, index=False)\n\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    train_file_paths = [\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2008.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2009.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2010.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2011.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2012.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2013.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2014.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2015.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2016.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2017.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2018.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2019.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2020.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2021.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2022.parquet',\n        '/kaggle/input/neo-bank-non-sub-churn-prediction/train_2023.parquet',\n    ]\n    \n    test_file_path = '/kaggle/input/neo-bank-non-sub-churn-prediction/test.parquet'\n    # Load Data\n    train_df = load_and_preprocess_data(train_file_paths)\n    test_df = load_and_preprocess_data([test_file_path])\n\n    #Target Variable\n    train_df = create_target_variable(train_df)\n\n    # Feature Engineering\n    train_df = feature_engineering(train_df)\n    test_df = feature_engineering(test_df)\n    \n    # Define Numerical and Categorical features\n    numerical_features = ['interest_rate', 'atm_transfer_in', 'atm_transfer_out', 'bank_transfer_in',\n                          'bank_transfer_out', 'crypto_in', 'crypto_out', 'bank_transfer_in_volume',\n                          'bank_transfer_out_volume', 'crypto_in_volume', 'crypto_out_volume', 'complaints',\n                           'tenure', 'days_since_start', 'days_since_last_activity', 'age','total_transfer_volume',\n                            'crypto_transfer_volume','volume_ratio']\n    \n    categorical_features = ['country', 'from_competitor', 'job', 'day_of_week', 'month', 'quarter', 'year']\n\n\n    for touchpoint in ['app', 'email', 'phone','chat']:\n        if f'touchpoint_{touchpoint}' in train_df.columns:\n          numerical_features.append(f'touchpoint_{touchpoint}')\n    \n    for channel in ['email', 'phone', 'app','chat']:\n       if f'csat_{channel}' in train_df.columns:\n         numerical_features.append(f'csat_{channel}')\n\n    #Create Preprocessing pipeline\n    preprocessor = get_preprocessing_pipeline(numerical_features, categorical_features)\n\n    # Model Training\n    model_all = train_models(train_df, preprocessor, numerical_features, categorical_features)\n    \n    # Prediction on test data\n    test_predictions = predict_with_orchestration(test_df, preprocessor, model_all)\n\n\n    # Create Submission\n    create_submission_file(test_df, test_predictions, 'submission.csv')\n    \n    print(\"Submission file created.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T12:32:33.627955Z","iopub.execute_input":"2025-06-17T12:32:33.628260Z","iopub.status.idle":"2025-06-17T12:32:33.653495Z","shell.execute_reply.started":"2025-06-17T12:32:33.628228Z","shell.execute_reply":"2025-06-17T12:32:33.652440Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}